### 오늘 한 것

#### ML 지도학습 scikitlearn

#### ML 전처리 & 분류모델의 성능측정


***

### 배운 내용

#### **L1  - 절댓값 - 라쏘**

- 극단적으로 실제 영향력이 있는 독립변수만을 골라줌
- 일부 특성의 가중치를 0으로 만들 수 있음
- 특정 특정을 선택하지 않게 만들어서 희소성을 높임
- **특성 선택**과 유사한 효과를 발휘

**L2 - 제곱 - 릿지**

- 고루고루 필드의 영향력을 회귀모델에 반영(제곱합)
- 계수를 0에 가깝게 만들
    - 계수가 정확히 0이 되지는 않음
    - 모든 특성에 어느 정도의 가중치가 주어짐
    - 과도한 가중치를 방지 → 모델을 일반화된 형태로 ,,

### SVM(Support Vector Machine)

- hyperplane
- 
    - decision boundary
        
        w^T x = 0
      

        
    - positive/negative hyperplane
        
        
        w^T x = +- 1
        
        
    - positive ~ negative hyperplane = margin
- support vectors
- radial basis function
    - 3차원으로 차원을 높였다가 새로 z축을 데이터 간 거리를 중심으로 계산
        
        → 많이 몰려있는 방향으로 3번째 축을 만들어서 새 평면으로 자름
        

장점: 꽤 괜찮은 성능

단점: 데이터가 많아지면 계산하는 시간이 오래 걸림, 설명하기 어려움

### 의사결정나무

- 특성 안에서 클래스를 절반에 가깝게 나누는 루트노드를 만들 때 무작위성이 적용됨
- 하이퍼파라미터를 지정해주지 않아도 설명할 수 있는 모델이 만들어짐
- 파라미터 안에서 특성별 중요도를 직관적으로 파악할 수 있게 됨

### 인스턴스 기반 vs 모델 기반

**모델 기반**

- 지금까지 거친 훈련 데이터의 특징을 문신처럼 남겨놓음
- w, b만 변하기 때문에 훈련을 많이 시킨다고 모델의 크기가 변하지 않음

**인스턴스 기반**

- 지금까지 거친 훈련 데이터를 기억함
- 훈련을 많이 시킬수록 모델의 크기가 커짐
- 훈련할 때 데이터를 모델에 올려놓고, 새 데이터가 들어오면 기존 데이터들과의 **최단거리**를 계산함

***

### KNN K-Nearest Neighbor

- 기하학적 거리 분류기
- 가장 가깝게 위치하는 멤버로 분류하는 방식
- hyperparameter인 K값은 근처에 참고할 이웃의 숫자
- 훈련 데이터셋을 메모리에 모두 저장하기 때문에 학습에 걸리는 시간이 없음
- 데이터의 개수가 늘어날수록 훈련 데이터셋에 할당하는 메모리가 커짐

### 모(모수, 비모수)

- **모** 어떠한 것에서 갈려 나오거나 생겨난 것의 근본이 됨
- **모수 모델** - 고정된 파라미터를 모델이 학습함에 따라 정교화
    - 퍼셉트론, 로지스틱회귀, SVM(선형)
- **비모수 모델** - 고정된 개수의 파라미터가 아니라 훈련 데이터가 늘어남에 따라 파라미터의 개수가 늘어남
    - SVM(커널형, 함수의 차원이 증가), KNN(데이터 증가), 결정트리/랜덤포레스트(노드 증가)

***

**과대적합 & 과소적합**

**과대적합**

- 모델이 학습데이터에만 과도하게 최적화
- 실제 예측을 다른 데이터로 수행할 경우 예측 성능이 과도하게 떨어지는 것

**과소적합**

- 모델이 너무 단순하여 학습 데이터를 충분히 학습하지 못함
- 데이터가 불충분하여 학습이 부족

**교차검증을 통해 과대적합, 과소적합을 방지할 수 있음**

### K-fold 교차 검증

- 데이터를 k등분 → 각 등분을 한번씩 테스트 데이터로 사용하여 성능 평가 & 평균값을 선택
- 데이터가 적은 경우 별도로 테스트데이터를 확보하면 비효율적
- 많은 데이터를 학습에 사용 & 성능 평가하는 방법이 필요
