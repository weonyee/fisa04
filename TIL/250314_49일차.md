## 오늘 한 것

### Transformer

### 교과목 평가


## 배운 내용
<aside>
💡

**단어 임베딩 = 단어를 실수 벡터로 표현**

</aside>

### 트랜스포머

- NLP 및 기타 시퀀스 데이터 처리에 매우 강력한 모델 구조
- **Self-attention 매커니즘**
    - 입력 시퀀스의 각 단어(토큰)가 다른 단어들과 얼마나 관련이 있는지 계산 → 각 단어에 대한 정보를 동적으로 생성
    - RNN과 달리 순차적인 계산을 하지 않아서 병렬 처리에 유리
- **병렬 처리**
    - RNN, LSTM과 달리 시퀀스의 모든 단어를 동시에 처리할 수 있어 학습 속도가 매우 빠름
- **인코더-디코더 구조**
    - **인코더** - 입력 시퀀스를 처리하여 중요한 정보(특징)을 압축
    - **디코더** - 인코더에서 나온 정보를 바탕으로 출력 시퀀스 생성
- **멀티헤드 어텐션**
    - 여러 개의 자기 주의 매커니즘을 병렬로 사용
    - 각기 다른 하위 공간에서 정보를 학습 & 이를 합쳐 더욱 풍부한 표현
- **위치 인코딩**
    - 순차적인 구조 X
    - 시퀀스 내 단어들의 순서 정보를 제공하기 위해 위치 인코딩 추가
- 예) BERT, GPT, T5

**트랜스포머의 주요 장점**

- **빠른 학습과 추론속도**: 병렬 처리
- **긴 시퀀스 처리**
- **유연성**: 여러 가지 NLP 작업에 맞춰 쉽게 변형하고 적용

***

### 허깅페이스

- 자연어 처리와 머신러닝 모델을 제공
- **오픈소스 라이브러리와 플랫폼**
- 트랜스포머 모델 및 관련 기술 제공

1. 트랜스포머 라이브러리
    - 자연어 처리 모델을 쉽게 사용할 수 있도록 해줌
    - `BERT, GPT-2, T5, RoBERTa, DistilBERT`
    - 모델 학습, fine-tuning(미세 조정), 추론(예측)을 쉽게 수행할 수 있도록 지원
    - `pytorch`나 `tensorflow` 등 딥러닝 프레임워크에서 사용할 수 있도록 지원
2. 모델 허브
    - pre-trained 모델을 제공하는 model hub
    - 감정 분석, 질문 응답, 기계 번역, 텍스트 생성
    - 오픈소스
3. Datasets 라이브러리
    - 데이터셋을 쉽게 불러오고, 전처리하고, 학습에 사용할 수 있도록 지원
4. Accelerate 라이브러리
    - 모델 학습을 더 빠르고 효율적으로 할 수 있도록 돕는 라이브러리
    - 분산 학습, 혼합 정밀도 훈련

**허깅 페이스의 주요 장점**

- 쉽고 빠른 사용
- 다양한 모델 지원
    - 유명한 모델뿐만 아니라 다양한 커스텀 모델까지 지원
- 커뮤니티와 협업
- 실험과 튜닝
    - 모델의 fine-tuning을 쉽게 할 수 있는 기능 제공

**제로샷 분류**

기계 학습 및 자연어 처리에서 각 개별 작업에 대한 특정 교육 없이 작업을 수행할 수 있는 모델 유형

NER(Named Entity Recognition) - 개체명 인식
