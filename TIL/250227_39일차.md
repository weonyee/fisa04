### 오늘 한 것

#### Regression

#### ensemble

***

### 공부한 내용

성능을 향상시키기 위해 하이퍼 파라미터 튜닝을 진행함

매번 손으로 하기 번거로우니까 → RandomizedSearchCV

#### 지도학습 - 회귀 분석

- 관찰된 **연속형 변수**들에 대해 두 변수 사이의 모형을 구한 뒤 적합도를 측정해 내는 분석 방법
- 단순 회귀 분석
    - 하나의 종속변수와 하나의 독립변수 사이의 관계를 분석
- 다중 회귀 분석
    - 하나의 종속변수와 여러 독립변수 사이의 관계를 규명하고자 할 경우


          from sklearn.linear_model import LinearRegression
          
          # 모델 인스턴스를 만든다
          li_model = LinearRegression()
          
          # 데이터를 fit 한다
          li_model.fit(x, y)
          
          # 데이터를 fit 한 모델로 predict 한다
          y_pred = li_model.predict(x)
          
          # 모델의 성능을 측정한다. score
          li_model.score(x, y)

          0.9831081424561687


>> 100개 중에 98개를 맞췄다는 뜻이 아니에유

오차를 누적해서 더하는 개념이기 때문에 0에 가까울수록 좋은 값(=커질수록 나쁜 성능)

1보다 더 커질 수도 있음

**R2 Score**

- 회귀분석에서 자주 쓰이는 회귀 평가 지표
- 실제 값의 분산 대비 예측 값의 분산 비율
- 1에 가까울수록 좋은 모델
- 음수가 나오면 잘못 평가됨

**MSE(Mean Squared Error)**

- 예측 값과 실제 값의 차이 제곱의 평균
- MSE 오차가 작으면 작을수록 좋지만, 과대적합인지 의심해보아야 함
- 예측 값과 실제 값보다 크게 예측이 되는지 작게 예측되는지 알 수 없음

**RMSE(Root Mean Squared Error)**

- 예측값과 실제값의 차이 제곱의 평균에 루트 씌움
- MSE 장단점을 거의 그대로 따라감
- 제곱 오차에 대한 왜곡을 줄여줌

**MAE(Mean Absolute Error)**

- 예측값과 실제값의 차이 절댓값의 평균
- 작을수록 좋지만 과대적합이 될 수 있음
- 스케일에 의존적
    - 10억 20억과 5000원 10000원의 MSE가 동일하게 100 → 동일한 오차율이 아님에도 동일하게 평가됨

**평균 절대 비율 오차(Mean Absolute Percentage Error)**

- (실제값 - 예측값) / 실제값
- 실제값에 대한 오차의 **비율**

시간이나 수집된 순서의 영향이 있다면 `shuffle = False` 사용

영향을 무시하려고 하면 `shuffle = True`

1. 데이터를 학습, 검증용으로 나눔
2. 모델 객체 생성
3. 학습
4. 검증

훈련데이터의 특성 개수가 늘어날수록 R2_score은 높아짐

변수가 늘어날수록 설명력이 커지기 때문에

#### Ensemble 앙상블

- 각각의 모델이 분류를 하고 그 분류의 평균을 내는 방식
- 여러 모델을 연결하여 더 강력한 모델을 만드는 기법
- 다수결(믿을만 하다)
- 더 많은 메모리를 동시에 차지하게 된다
- 종류
    - 배깅(independent weak learner training): 조금씩 다른 데이터를 입력, 병렬적
    - 부스팅(iterative weak learner training): 똑같은 데이터를 넣어줌 → 못 맞추는 것들을 더 크게 해서 다음 레벨로 → 반복
- Voting
    - hard voting - 다수결
    - soft voting - 각각 분류한 확률값을 기반으로 평균 확률값을 내어서 더 높은 확률을 갖는 클래스로 최종 선정

#### 배깅 - 랜덤 포레스트

**randomforestclassifier**

- 매개변수 튜닝 없이도 선형 모델이나 단일 결정 트리보다 높은 결과치를 제공하기도 함
- 배깅(Bootstrap Aggregation) 방식 적용
- 여러 개의 같은 모델한테 부트스트랩 방식(샘플의 분포를 약간만 변형하면서 복원추출하는 방식)으로 비슷하지만 일부 다른 표본을 병렬적 학습
- 같은 모델이지만 조금씩 다른 분포를 학습하므로 과적합 방지 가능

#### Boosting - GBM(Gradient Boosting Machine)

- 경사하강법에 기반하여 부스팅하는 기법
- 순차적으로 분류기들을 학습하기 때문에 학습시간이 상대적으로 오래 걸린다는 단점
