### 오늘 배운 것
- pytorch

***

### 배운 내용

    # 1. 신경망
    class Net(nn.Module):
        def __init__(self):
            # 신경망 구성 요소 정의
    
        def forward(self, input):
            # 신경망의 동작 정의
            return output
    
    
    # 2. 데이터셋
    class Dataset():
        def __init__(self):
            # 필요한 데이터 불러오기
    
        def __len__(self):
            # 데이터의 개수 반환
            return len(data)
    
        def __getitem__(self, i):
            # i번째 입력 데이터와 i번째 정답을 반환
            return data[i], label[i]
    
    
    # 3. 딥러닝 학습
    for data, label in DataLoader():
        # 모델의 예측값 계산
        prediction = model(data)
    
        # 손실함수를 사용해 오차 계산
        loss = LossFunction(prediction, label)
    
        # 오차 역전파
        loss.backward()
    
        # 신경망 가중치 수정
        optimizer.step()


**SGD(확률적 경사 하강법)**

- 랜덤하게 추출한 일부 데이터를 사용해 더 빨리, 자주 업데이트 하게 하는 것
- **전부 다 봐야함, 한 걸음은 너무 오래 걸리니까 조금만 보고 빨리 판단, 같은 시간에 더 많이 감**
- 속도 개선

**모멘텀**

- 관성의 방향을 고려해 진동과 폭을 줄이는 효과
- **스텝 계산해서 움직인 후, 아까 내려 오던 관성 방향 또 가자**
- 정확도 개선

**NAG(네스테로프 모멘텀)**

- 모멘텀이 이동시킬 방향으로 미리 이동해서 그래디언트 계산
- 불필요한 이동을 줄이는 효과
- **일단 관성 방향 먼저 움직이고, 움직인 자리에 스텝을 계산하니 더 빠름**
- 정확도 개선

**Adagrad**

- 변수의 업데이트가 잦으면 학습률을 적게 하여 이동 보폭을 조절하는 방법
- **안 가본 곳은 성큼 빠르게 훑고 많이 가본 곳은 잘 아니까 갈수록 보폭을 줄여 세밀히 탐색**
- 보폭 크기 개선

**RMSProp**

- Adagrad의 보폭 민감도를 보완한 방법
- **보폭 줄이는 건 좋은데 이전 맥락 상황 봐가며**
- 보폭 크기 개선

**Adam**

- 모멘텀과 RMSProp을 합친 방법
- **방향도 스텝 사이즈도 적절하게**
- 정확도와 보폭 크기 개선

**활성화 함수**

- 입력을 활성화해서 다양한 출력을 구성
- 입력의 총합을 어떻게 활성화해서 출력하는지를 결정하는 함수
- 각 노드가 이전 노드들로부터 전달받은 정보를 다음 노드에 얼마만큼 전달해줄지를 결정
- 가중치 값을 학습할 때 에러가 적게 나도록 도와주는 함수

> 선형함수를 활성화함수로 사용하면 안되는 이유?

- 여러 층으로 된 모델을 하나의 층만으로도 나타낼 수 있게 되기 때문
    - 활성화 함수가 선형 함수이면 여러 층의 레이어를 행렬의 곱으로 나타낼 수 있어 하나의 행렬과 같아짐
- vanishing gradient 문제
    - 역전파에서 레이어가 깊어지며 업데이트가 사라지면서 underfitting 발생

### Optuna로 최적의 값을 찾기

- 406개의 데이터는 딥러닝 네트워크를 학습시키기엔 작음
- 늘 딥러닝이 성능이 좋지는 않음
- 이런 경우 머신러닝이 나음
- 빅데이터의 기준(딥러닝을 시도해볼만하다) = 10,000개
