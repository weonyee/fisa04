### 오늘 한 것

#### ML 전처리 / 분류모델 성능

#### 기술 세미나 준비

***

### 정확도

accuracy = (TP+TN)/(TP+TN+FP+FN)

- (직관적으로) 모델의 성능을 나타낼 수 있는 평가지표
- 실제 데이터와 예측 데이터가 얼마나 같은지 평가

### 정밀도

precision = TP/(FP+TP)

- 양성으로 판단한 것 중 진짜 양성의 비율

### 재현율

recall = TP/(FN+TP)

- 진짜 양성인 것들 중에서 올바르게 양성으로 판단한 비율
- sensitivity랑 같음
- 모델의 안정성을 평가하는 지표

### 정밀도와 재현율 = trade-off

- 재현율과 정밀도 모두 TP를 높이는 데 초점을 맞춤
- 재현율은 FN을 낮추는 데, 정밀도는 FP를 낮추는 데 초점을 맞춤

### F1 score

- 정밀도와 재현율을 결합한 지표
- 어느 한쪽으로 치우치지 않을 때 높은 값을 가짐

$$
\begin{equation}F1 = 2 \times \frac{precision \times recall}{precision + recall}\end{equation}
$$

### ROC 곡선과 AUC

- ROC 곡선은 FPR(False Positive Rate)이 변할 때 TPR(True Positive Rate)이 어떻게 변하는지 나타내는 곡선
- **AUC(Area Under Curve) 값은 ROC 곡선 밑에 면적을 구한 값 (1이 가까울수록 좋은 값)**

### AUC의 장점

- AUC는 척도 불변
    - 절댓값이 아닌, 예측이 얼마나 잘 평가되었는지를 측정
- 분류 임계값 불변
    - 어떤 분류 임계값이 선택되었는지와 무관하게 모델의 예측 품질을 측정

### classification_report

macro: 모든 라벨(클래스)에 동등한 중요도를 부여해서 출력하는 평균 점수

- 가장 빈도 높은 클래스 레이블의 성능이 중요할 때 사용

weighted: 라벨별로 해당되는 샘플의 개수를 감안해서 중요도를 다르게 매겨 출력

micor: 각 샘플이나 예측에 동일한 가중치를 부여해 클래스별로 TP, TN, FP, FN을 계산

### pycaret: low-code machine learning

- 머신러닝 워크플로우를 자동화하는 오픈소스 파이썬 라이브러리

**주요 특징**

1. 자동화: 전처리, 모델 선택, 하이퍼파라미터 튜닝 등
2. 다양한 작업 지원: 분류, 회귀, 클러스터링
3. 모델 비교: `compare_models()`
4. 모델 최적화: `tune_model()`
5. 시각화: `plot_model()`
6. 앙상블 모델: `blend_models()`

**Setup ➡️ Compare Models ➡️ Analyze Model ➡️ Prediction ➡️ Save Model**

imputation - 결측치 처리 옵션

## 은행 마케팅 데이터셋을 활용한 분류기 만들기

- 머신러닝에 넘기기 위해 데이터 타입을 object → int
