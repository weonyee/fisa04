### 오늘 한 것

#### pytorch

#### CNN

***

### 배운 내용

모델 차원

- 층의 개수를 늘린다.
- 층마다 뉴런의 개수를 늘린다.
- lr을 크게 준다.
- optimizer를 바꾼다.

데이터 차원

- pca 등 차원을 줄일 수 있는 방법을 고민한다.
- 정규화(시도) 이미지는 0~255 정규화 상태이기 때문에 필요 없음.

**파이토치의 특징**

- 동적 계산 그래프
    - 실행 중에 연산 그래프를 만들고 수정할 수 있어서 디버깅이 쉬움
    - 텐서플로우의 `static graph`보다 유연함
- Numpy와 유사한 API
    - `torch.Tensor`가 Numpy의 배열과 비슷해서 직관적
    - 넘파이처럼 `.numpy()`를 사용해서 변환 가능
- GPU 지원 (CUDA & MPS)
    - `torch.cuda`를 이용한 GPU 가속 연산 지원
    - mac에서 torch.mps로 가속
- 자동미분 AutoGrad
    - `torch.autograd`를 사용하면 자동으로 미분 계산
    - 신경망의 역전파를 쉽게 구현 가능
- 모델 구축이 직관적
    - `torch.nn.Module`을 사용해 신경망을 객체지향적으로 설계 가능
    - 텐서플로우의 Keras처럼 간결하고 직관적인 코드 작성 가능

사용하는 곳: 딥러닝 연구, 컴퓨터 비전, 자연어 처리, 강화 학습, 생성 모델

### pytorch 모델 생성

- (기본적) `torch.nn.Module`을 상속받아 클래스 정의

```jsx
import torch
import torch.nn as nn

# 1. 모델 정의
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MyModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)  # 첫 번째 선형 레이어
        self.relu = nn.ReLU()  # 활성화 함수
        self.fc2 = nn.Linear(hidden_size, output_size)  # 두 번째 선형 레이어

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# 2. 모델 인스턴스 생성
model = MyModel(input_size=10, hidden_size=20, output_size=1)

# 3. 모델 구조 확인
print(model)

```

- `__init__()`에서 필요한 레이어(`Linear`, `ReLU`) 정의
- 옵티마이저 설정 → `Adam` 사용, 학습률 `lr` 지정

### 손실을 측정하는 방법

- 평균제곱오차 mean squared error
- 각 w, b를 어떤 최적화 방법으로 변경할지 정해서 모델에 넘겨줌
- 손실함수 `MSELoss()` 사용(회귀 문제)
    - 손실함수: 모델의 예측값과 실제값(정답) 사이의 차이를 측정하는 함수
    - 손실이 작을수록 모델의 예측이 정확하다는 의미
    - 손실을 최소화하는 방향으로 학습이 진행됨
    - 크게 회귀와 분류 문제에 따라 다르게 사용됨
    - `torch.nn` 모듈에서 손실함수 제공

**손실함수 - 회귀모델**

- MSE(Mean Sqaured Error) 평균제곱오차
    - 가장 많이 사용됨
    - 오류(차이값)를 제곱하여 평균을 계산
- MAE(Mean Absolute Error) 평균절대오차
    - 차이를 절댓값으로 변호나 후 평균 계산
    - 이상치에 덜 민감
- Huber loss
    - MSE와 MAE의 장점을 결합
    - 이상치에 덜 민감한 손실
    - 작은 오차일 때는 MSE처럼 동작
    - 큰 오차일 때는 MAE처럼 동작(이상치 영향 줄임)
    - 이상치에 대한 robust가 필요할 때 사용

**손실함수 - 분류**

- CEE(Cross Entropy Loss) 교차 엔트로피 손실
    - 가장 많이 사용됨
    - 확률 기반 손실(softmax)
- negative log likelihood loss(NLLLoss)
    - 로그 확률을 기반으로 손실 계산
    - softmax 없이 사용
- BCE(Binary Cross Entropy)
    - 2진 분류 문제에서 사용

**배치**

- 한 번에 학습하는 데이터의 수
- 전체 데이터셋을 한번에 학습하지 않고 여러 개의 작은 묶음(Batch)로 나누어서 학습
- 배치 사이즈가 작으면 학습이 빠르게 진행되지만, 학습이 불안정함
- 배치 사이즈가 크면 학습이 안정적이지만 메모리를 많이 사용


### 가중치 초기화

- 모델의 초기 가중치 값을 설정하는 방법
- 빨리 최적화 지점으로 도달할 수 있음
- 모델 매개변수에 적절한 초기값을 설정시 기울기 폭주/소실 문제를 완화시키고 수렴 속도를 향상시켜 전반적인 학습 프로세스 개선이 가능
- 상수 초기화
    - 0, 1 등의 매우 작은 양의 상숫값으로 모든 가중치를 동일하게 할당
    - 역전파 과정에서 모든 가중치가 동일한 값으로 갱신되어 모델이 학습되지 않음
- 자비에르(글로럿) 초기화
    - 균등분포나 정규분포를 사용하여 은닉층의 노드 수에 따라 다른 표준 편차를 할당
    - 입력 데이터의 분산이 출력 데이터에서 유지되도록 가중치를 초기화
    - 시그모이드/하이퍼볼릭 탄젠트 활성화함수를 사용하는 경우 효과적
- 카이밍(허) 초기화
    - 현재 계층의 입력 뉴런 수만을 기반으로 노드의 출력 분산이 입력 분산과 동일하도록 가중치 초기화
    - ReLU(죽은 뉴런 문제 최소화), 순방향 네트워크에서 효과적
- 그 외: 균등분포, 정규분포 초기화 등

### L1, L2 규제 적용

- 규제 강도가 0에 가까워질수록 모델은 더 많은 특징을 사용하므로 과대 적합에 민감
- 규제 강도를 높이면 과소 적합 문제에 노출

**L1 (Lasso 규제, 절댓값 규제)**

- L1 정칙화를 적용 → 입력 데이터에 더욱 민감해짐
- 모델의 가중치를 정확히 0으로 만드는 경우가 있어서 희소한 모델이 될 수 있음
- 불필요한 특징을 처리하지 않아 모델 성능이 올라갈 수도 있음 but 예측에 사용되는 특징 수가 줄어들어 정보 손실의 우려가 있음
- 미분 불가로 인해 역전파 계산에 리소스 소모(주로 선형 모델에 적용)

**L2 (Ridge 규제, 제곱합 규제)**

- 벡터 또는 행렬 값이 크기를 계산하여 손실함수에 가중치의 제곱합을 추가해 과대적합을 방지하도록 규제
- 하나의 특징이 너무 중요한 요소가 되지 않도록 규제를 가하는 것에 의미를 둠
- 가중치 값들이 비교적 균일하게 분포되며, 가중치는 0이 되지는 않고 가까워짐

**가중치 감쇠**

- 머신러닝과 딥러닝에서 과적합을 방지하기 위해 사용되는 정규화 기법 중 하나
- L2 정칙화를 옵티마이저에 제공하여 구현
    - 옵티마이저에 제시하면 손실함수에 규제를 부여하게 됨
- 모델을 훈련할 때, 가중치 감쇠를 추가 적용하면 가중치가 클수록 더 큰 페널티를 부여 → 모델은 가중치를 작게 만드려고 노력함

### 컨볼루션 연산

기존의 이미지 인식 딥러닝 모델은 행렬을 정형데이터의 행으로 flatten하는 과정에서 고유 특성인 공간적 정보를 잃게 됨

<aside>
💡

CNN은 이미지 그대로의 특징을 딴 Convolution 필터를 앞에 덧댐으로서 입력의 차원은 줄이고, 공간 특성은 그대로 받을 수 있게 만든 모델

</aside>

**필터 연산**

- 입력 데이터에 필터를 통해 필터에 대응하는 원소끼리 곱하고, 그 합을 구함
- 연산이 완료된 결과 데이터를 특징 맵이라 부름

**필터** 

- 커널이라고도 함
- 필터의 사이즈는 거의 홀수(짝수면 패딩이 비대칭이 됨)
- 과적합 방지

**풀링**

- 필터(커널) 사이즈 내에서 특정 값을 추출하는 과정

**맥스 풀링**

- 가장 많이 사용
- 출력 데이터의 사이즈는 컨볼루션 연산과 동일
- 일반적으로 `stride=2, kernel_size=2` : 특정 맵의 크기를 절반으로 줄이는 역할
- 모델이 물체의 주요한 특징을 학습할 수 있도록 해줌
- 컨볼루션 신경망이 이동 불변성 특성을 가지게 함
- 모델의 파라미터 개수를 줄여주고 연산 속도를 빠르게 함
