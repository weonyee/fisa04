### 오늘 한 것

#### Ensemble

#### 비지도학습_PCA

#### 지도학습_LDA

***

### 배운 내용


- 다중 선형 회귀 모델은 필요 이상으로 자세하게 특징값과 라벨값의 관계를 분석 → 일반화 능력이 떨어져 새로운 데이터를 제대로 예측하지 못하게 되는 과적합 경향 有

**XGBoost(eXtra Gradient Boosting)**

- 병렬처리가 불가능한 GBM의 단점을 보완
- GPU 지원 가능 & 추가적인 정규화 & Tree pruning & Early Stopping, 내장된 교차검증, 결측치 처리
- GBM보다 속도 굿, 과적합 방지 가능

XGBoost 조기 중단

1. fit() 함수에 조기중단 파라미터 입력
2. 평가 지표가 향상될 수 있도록 반복횟수를 정의하는 `early_stopping_rounds` /  조기 중단을 위한 평가 지표인 `eval_metric` / 성능 평가를 수행할 데이터 셋 `eval_set`
3. eval_set으로 반복 수행마다 지정된 데이터셋에 eval_metric으로 지정된 평가 지표 함수로 예측 오류 측정

**LightGBM**

- 대용량 처리에 적합
- 병렬처리인 GPU 지원
- 빠른 속도가 장점 but 과적합 문제를 일으키키도
- 메모리 사용량이 적음
- **리프 중심 트리 분할**
    - 최대 손실값을 가지는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적으로 생성
    - 리프 노드를 분할해 생성된 규칙 트리는 학습을 반복할수록 균형 트리 방식보다 예측 오류 손실을 최소화
- 동일한 데이터셋이어도 양이 적거나, 하이퍼 파라미터 값에 따라 이론과 동일한 결과치를 보장할 수 없음 → 상황에 따라 다양한 API 비교 및 검증 필수

### 비지도 학습

**차원의 저주**

차원이 증가할수록 개별 차원 내 학습할 데이터 수가 적어지는 sparse(희소) 현상

→ 학습이 제대로 이루어지지 않음

→ 해결: 차원을 줄이거나 데이터를 많이 획득해야 함

→ 차원이 커질수록 연산량이 기하급수적으로 증가하기 때문에 효율 측면에서 차원의 저주는 경계해야 함

**주성분 분석**

- 여러 변수 간에 존재하는 상관관계를 이용해 주성분을 추출해 차원을 축소하는 기법
- **차원 축소 기법**
- 차원 축소 → 기존 데이터의 정보 유실이 최소화

1. 학습 데이터셋에서 분산이 최대인 축(1축)을 찾음
2. 1축과 직교하면서 분산이 최대인 2축을 찾음
3. 1축과 2축에 직교하고 분산을 최대한 보존하는 3축을 찾음
4. 1~3.과 같은 방법으로 차원 수 만큼의 축을 찾음

PCA는 데이터를 선형으로 계산할 수 있는 축을 N개 만듦

비선형으로 패턴을 가진 데이터를 분류하기에 애매한 지점이 생김

커널 기법을 사용해서 데이터를 더 높은 차원으로 보내 직선을 찾음

→ 낮은 차원으로 바꿔서 차원을 축소함

**매니폴드 학습**

- 다양체라고도 하며 국소적으로 유클리드 공간과 닮은 위상 공간
- 시각화를 목적으로 하여 2개 정도의 특성을 뽑음

**고유얼굴(eigenface) 특성 추출**

- PCA를 이용해서 LFW 데이터셋의 얼굴 이미지에서 특성을 추출
- 2000년 초반 이후의 정치인, 가수, 배우, 운동선수들의 얼굴을 포함

### 지도학습_LDA

**주성분 분석 VS 선형 판별 분석**

- 차원의 저주로 인한 과대 적합 정도를 줄이기 위한 특성 추출 기법 중 하나
    
    PCA
    
    - 분산이 최대인 직교 성분 축을 찾아 해당 축으로 데이터를 사영
    - 비지도 학습
    - 데이터 차원을 줄이기 위해 데이터가 안겹치게 내릴 수 있는 화살표를 찾음(공분한 행렬 활용)
    
    LDA
    
    - 클래스를 최적으로 구분할 수 있는 특성 부분 공감을 찾음
    - 지도학습
    - 입력 데이터의 클래스(정답)를 최대한 분리할 수 있는 축을 찾음
    - 클래스의 분포를 가장 잘 분류할 수 있는 N개의 축으로 데이터 특성을 추출
    - 축이 많은 경우에 유효(그렇지 않은 경우에는 유의미한 결과 X)
    
    특성 추출
    
    - PCA, LDA 데이터를 새로운 방식으로 변환해서 중요한 정보를 담은 또 다른 변수를 만들어 사용하는 것
    
    특성 축소
    
    - 기존 변수 중 의미 있는 변수만 남기고 걸러내는 것

### 비지도학습_Clustering, K-means

**K-means 군집화 알고리즘**

- ML 비지도학습
- 데이터를 k개의 군집으로 묶는 알고리즘
- K-means vs K-NN
    - K-means - **clustering**: 레이블(정답)이 주어지지 않았을 때 주어진 데이터를 묶은 clustering(비지도 학습)
    - K-NN - **classification** : k개의 점을 지정하여 거리를 기반으로 구현되는 거리기반 분석 알고리즘
        - 차이점: 이웃과의 거리를 측정하여 새 데이터의 클래스를 결정하는 KNN(지도학습)

**Clustering**

1. centroids를 랜덤한 자리에 배치
2. 각 데이터를 centroids와 거리를 계산해서 가까운 쪽에 군집화
3. 다시 centroids를 군집의 중심으로 위치를 옮김
4. centroids로부터 해당 군집에 포함된 데이터의 거리를 연산
5. 수렴할 때까지 반복

- 목표: 유사한 데이터는 같은 그룹, 다른 데이터는 다른 그룹으로 분류하는 것이 목적
    - 몇 개의 그룹으로 묶을까?
    - 유사함의 기준은 무엇인가?(분포가 중심으로부터 얼마나 쏠려있는가)
